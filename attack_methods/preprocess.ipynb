{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:50<00:00,  9.91it/s]\n",
      "100%|██████████| 500/500 [01:19<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food:  53  || other:  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 62843.55 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m     save \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mconcatenate_datasets([\n\u001b[1;32m     40\u001b[0m         datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(target),\n\u001b[1;32m     41\u001b[0m         datasets\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(other)\n\u001b[1;32m     42\u001b[0m     ])\n\u001b[1;32m     43\u001b[0m     save\u001b[38;5;241m.\u001b[39msave_to_disk(output_path)\n\u001b[0;32m---> 44\u001b[0m target, other \u001b[38;5;241m=\u001b[39m preprocess_classify(output_path, data, classifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfood\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# model, tokenizer = load_model(\"meta-llama/Meta-Llama-3-8B-Instruct\", AutoModelForCausalLM)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# gen_conf = get_gen_config(tokenizer)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#few shot evaluation, is this related to McDonald's, might need some human annotators, like 20 i think to compare. \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, set_seed\n",
    "import numpy as np\n",
    "from utils import generate_for, load_model, get_gen_config\n",
    "from prompts import MCDONALDS_PROMPT\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "output_path = '/nas03/terry69/multiturn/preprocess/'\n",
    "set_seed(42)\n",
    "data = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split = \"train_sft[:50%]\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", batch_size=512, device_map=\"auto\", max_memory={i: f\"{20}GiB\" for i in range(4)})\n",
    "candidate_labels = ['food', 'other']\n",
    "\n",
    "#for viz purposes\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, ls):\n",
    "        self.ls = ls\n",
    "    def __len__(self):\n",
    "        return len(self.ls)\n",
    "    def __getitem__(self, i):\n",
    "        return self.ls[i]\n",
    "\n",
    "#ensure sequence == instance[3][messages]\n",
    "def preprocess_classify(output_path, data, classifier, target_label):\n",
    "    target, other, scores = [], [], []\n",
    "    process = [instance['messages'][3]['content'] if len(instance['messages']) >= 3 else instance['messages'][1]['content'] for instance in data ]\n",
    "    mydata = MyDataset(process)\n",
    "    for out in tqdm(classifier(mydata, candidate_labels), total=len(mydata)):\n",
    "        scores.append(out)\n",
    "    for i, instance in enumerate(tqdm(data['messages'])):\n",
    "        if len(instance) >= 3 and scores[i]['labels'][np.argmax(scores[i]['scores'])] == target_label:\n",
    "                assert scores[i]['sequence'] == instance[3]['content']\n",
    "                target.append({'message':instance, \"prompt\": data['prompt'][i]})\n",
    "        else:\n",
    "            other.append({'message':instance, \"prompt\": data['prompt'][i]})\n",
    "    print(\"food: \",len(target), \" || other: \", len(other))\n",
    "    save = datasets.concatenate_datasets([\n",
    "        datasets.Dataset.from_list(target),\n",
    "        datasets.Dataset.from_list(other)\n",
    "    ])\n",
    "    save.save_to_disk(output_path)\n",
    "preprocess_classify(output_path, data, classifier, \"food\")\n",
    "# model, tokenizer = load_model(\"meta-llama/Meta-Llama-3-8B-Instruct\", AutoModelForCausalLM)\n",
    "# gen_conf = get_gen_config(tokenizer)\n",
    "\n",
    "# text = \"what should I get to eat on saturday lunch?\"\n",
    "# sentiment_task = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "#classify the second utterance as having food or not\n",
    "\n",
    "#     text = generate_for(text, MCDONALDS_PROMPT, tokenizer, model, gen_conf)\n",
    "# print(text)\n",
    "\n",
    "#few shot evaluation, is this related to McDonald's, might need some human annotators, like 20 i think to compare. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
